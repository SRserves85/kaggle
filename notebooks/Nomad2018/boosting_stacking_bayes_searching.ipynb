{
  "cells": [
    {
      "metadata": {
        "_uuid": "a8c77cfd21cca4e8add359b00a8ee60a15929d29",
        "_cell_guid": "d7c70224-e72c-44b2-9318-ef890ca3f117"
      },
      "cell_type": "markdown",
      "source": "# Introduction\n\nI created this notebook to show what I have learned with stacking and boosting models.\nI currently work as a data scientist, but most of my projects don't involve boosting or stacking."
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "56752974bed1473fe1eff3751e4028e4d6f04edd",
        "_cell_guid": "e3f7fae9-25ac-4b35-b1bc-678349cb811f",
        "trusted": false
      },
      "cell_type": "code",
      "source": "% matplotlib inline\n\nimport matplotlib.pyplot as plt\n\nimport os\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a949698ab8c042d1a2bee3be7815b75eb291827c",
        "_cell_guid": "004d5e5d-2e05-4987-95ea-5b7e2199f12e"
      },
      "cell_type": "markdown",
      "source": "# Loading the data\n\nI do not have any domain knowledge of this challenge, but I have been reading and viewing other kernels.\nThe following loading and renaming of features is not my own work."
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "e25308fc6dd2bb96f7c887d2c093d9d6b77dc49a",
        "_cell_guid": "4a55393d-3c47-4431-bb41-f128b86b696a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "test = pd.read_csv('../input/nomad2018-predict-transparent-conductors/test.csv')\ntest_id = test.id\n\ntrain = pd.read_csv('../input/nomad2018-predict-transparent-conductors/train.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fff8fd2bbab711ab34df4baf404c22da72d96acc",
        "_cell_guid": "ab9b39d5-0059-41e4-b167-5d1c694d2a32"
      },
      "cell_type": "markdown",
      "source": "The following was taken from another kaggler's script, but adds atomic coordinates"
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "collapsed": true,
        "_uuid": "a453c34ca15ad72c7834735a99b43e9acf0fc15c",
        "_cell_guid": "78e1d0ea-f832-4719-bd59-645571858a22",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# get coordinate information\n\nga_cols = []\nal_cols = []\no_cols = []\nin_cols = []\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfor i in range(6):\n    ga_cols.append(\"Ga_\"+str(i))\n\nfor i in range(6):\n    al_cols.append(\"Al_\"+str(i))\n\nfor i in range(6):\n    o_cols.append(\"O_\"+str(i))\n\nfor i in range(6):\n    in_cols.append(\"In_\"+str(i))\n\n\n\nga_df= pd.DataFrame(columns=ga_cols)\nal_df = pd.DataFrame(columns=al_cols)\no_df = pd.DataFrame(columns= o_cols)\nin_df = pd.DataFrame(columns=in_cols)\n\ndef get_xyz_data(filename):\n    pos_data = []\n    lat_data = []\n    with open(filename) as f:\n        for line in f.readlines():\n            x = line.split()\n            if x[0] == 'atom':\n                pos_data.append([np.array(x[1:4], dtype=np.float),x[4]])\n            elif x[0] == 'lattice_vector':\n                lat_data.append(np.array(x[1:4], dtype=np.float))\n    return pos_data, np.array(lat_data)\n\n\n\nfor i in train.id.values:\n    fn = \"../input/nomad2018-predict-transparent-conductors/train/{}/geometry.xyz\".format(i)\n    train_xyz, train_lat = get_xyz_data(fn)\n    \n    ga_list = []\n    al_list = []\n    o_list = []\n    in_list = []\n    \n    for li in train_xyz:\n        try:\n            if li[1] == \"Ga\":\n                ga_list.append(li[0])\n        except:\n            pass\n        try:\n            if li[1] == \"Al\":\n                al_list.append(li[0])\n        except:\n            pass\n        try:\n            if li[1] == \"In\":\n                in_list.append(li[0])\n        except:\n            pass\n        try:\n            if li[1] == \"O\":\n                o_list.append(li[0])\n        except:\n            pass\n    try:\n        model = PCA(n_components=2)\n        ga_list = np.array(ga_list)\n        temp_ga = model.fit_transform(ga_list.transpose())\n        temp_ga = [item for sublist in temp_ga for item in sublist]\n       \n    except:\n        temp_ga = [0,0,0,0,0,0]\n#         print i\n    try:\n        model = PCA(n_components=2)\n        al_list = np.array(al_list)\n        temp_al = model.fit_transform(al_list.transpose())\n        temp_al = [item for sublist in temp_al for item in sublist]\n#         print i\n    except:\n        temp_al = [0,0,0,0,0,0]\n#         print i\n    try:\n        model = PCA(n_components=2)\n        o_list = np.array(o_list)\n        temp_o = model.fit_transform(o_list.transpose())\n        temp_o = [item for sublist in temp_o for item in sublist]\n#         print i\n    except:\n        temp_o = [0,0,0,0,0,0]\n#         print i\n    \n    try:\n        model = PCA(n_components=2)\n        in_list = np.array(in_list)\n        temp_in = model.fit_transform(in_list.transpose())\n        temp_in = [item for sublist in temp_in for item in sublist]\n#         print i\n    except:\n        temp_in = [0,0,0,0,0,0]\n#         print i\n\n    temp_ga = pd.DataFrame(temp_ga).transpose()\n    temp_ga.columns = ga_cols\n    temp_ga.index = np.array([i])\n\n    temp_al = pd.DataFrame(temp_al).transpose()\n    temp_al.columns = al_cols\n    temp_al.index = np.array([i])\n\n    temp_o = pd.DataFrame(temp_o).transpose()\n    temp_o.columns = o_cols\n    temp_o.index = np.array([i])\n    \n    temp_in = pd.DataFrame(temp_in).transpose()\n    temp_in.columns = in_cols\n    temp_in.index = np.array([i])\n    \n    \n\n    ga_df = pd.concat([ga_df,temp_ga])\n    al_df = pd.concat([al_df,temp_al])\n    o_df = pd.concat([o_df,temp_o])    \n    in_df = pd.concat([in_df,temp_in])\n    \nga_df[\"id\"] = ga_df.index\nal_df[\"id\"] = al_df.index\no_df[\"id\"] = o_df.index\nin_df[\"id\"] = in_df.index\n\ntrain = pd.merge(train,ga_df,on = [\"id\"],how = \"left\")\ntrain = pd.merge(train,al_df,on = [\"id\"],how = \"left\")\ntrain = pd.merge(train,o_df,on = [\"id\"],how = \"left\")\ntrain = pd.merge(train,in_df,on = [\"id\"],how = \"left\")\n\nga_df= pd.DataFrame(columns=ga_cols)\nal_df = pd.DataFrame(columns=al_cols)\no_df = pd.DataFrame(columns= o_cols)\nin_df = pd.DataFrame(columns=in_cols)\n\nfor i in test.id.values:\n    fn = \"../input/nomad2018-predict-transparent-conductors/test/{}/geometry.xyz\".format(i)\n    test_xyz, test_lat = get_xyz_data(fn)\n    \n    ga_list = []\n    al_list = []\n    o_list = []\n    in_list = []\n    \n    for li in test_xyz:\n        try:\n            if li[1] == \"Ga\":\n                ga_list.append(li[0])\n        except:\n            pass\n        try:\n            if li[1] == \"Al\":\n                al_list.append(li[0])\n        except:\n            pass\n        try:\n            if li[1] == \"In\":\n                in_list.append(li[0])\n        except:\n            pass\n        try:\n            if li[1] == \"O\":\n                o_list.append(li[0])\n        except:\n            pass\n    \n#     ga_list = [item for sublist in ga_list for item in sublist]\n#     al_list = [item for sublist in al_list for item in sublist]\n#     o_list = [item for sublist in o_list for item in sublist]\n   \n    \n    try:\n        model = PCA(n_components=2)\n        ga_list = np.array(ga_list)\n        temp_ga = model.fit_transform(ga_list.transpose())\n        temp_ga = [item for sublist in temp_ga for item in sublist]\n       \n    except:\n        temp_ga = [0,0,0,0,0,0]\n#         print i\n    try:\n        model = PCA(n_components=2)\n        al_list = np.array(al_list)\n        temp_al = model.fit_transform(al_list.transpose())\n        temp_al = [item for sublist in temp_al for item in sublist]\n#         print i\n    except:\n        temp_al = [0,0,0,0,0,0]\n#         print i\n    try:\n        model = PCA(n_components=2)\n        o_list = np.array(o_list)\n        temp_o = model.fit_transform(o_list.transpose())\n        temp_o = [item for sublist in temp_o for item in sublist]\n#         print i\n    except:\n        temp_o = [0,0,0,0,0,0]\n#         print i\n    \n    try:\n        model = PCA(n_components=2)\n        in_list = np.array(in_list)\n        temp_in = model.fit_transform(in_list.transpose())\n        temp_in = [item for sublist in temp_in for item in sublist]\n#         print i\n    except:\n        temp_in = [0,0,0,0,0,0]\n#         print i\n\n    temp_ga = pd.DataFrame(temp_ga).transpose()\n    temp_ga.columns = ga_cols\n    temp_ga.index = np.array([i])\n\n    temp_al = pd.DataFrame(temp_al).transpose()\n    temp_al.columns = al_cols\n    temp_al.index = np.array([i])\n\n    temp_o = pd.DataFrame(temp_o).transpose()\n    temp_o.columns = o_cols\n    temp_o.index = np.array([i])\n    \n    temp_in = pd.DataFrame(temp_in).transpose()\n    temp_in.columns = in_cols\n    temp_in.index = np.array([i])\n    \n    \n\n    ga_df = pd.concat([ga_df,temp_ga])\n    al_df = pd.concat([al_df,temp_al])\n    o_df = pd.concat([o_df,temp_o])    \n    in_df = pd.concat([in_df,temp_in])\n    \n\nga_df[\"id\"] = ga_df.index\nal_df[\"id\"] = al_df.index\no_df[\"id\"] = o_df.index\nin_df[\"id\"] = in_df.index\n\ntest = pd.merge(test,ga_df,on = [\"id\"],how = \"left\")\ntest = pd.merge(test,al_df,on = [\"id\"],how = \"left\")\ntest = pd.merge(test,o_df,on = [\"id\"],how = \"left\")\ntest = pd.merge(test,in_df,on = [\"id\"],how = \"left\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "collapsed": true,
        "_uuid": "6bd5fef0beeb7ee49ba3a17c88607fd94170078b",
        "_cell_guid": "681ddff2-f247-403f-8429-3dc1113a307f",
        "trusted": false
      },
      "cell_type": "code",
      "source": "train.rename(columns={\n    'spacegroup' : 'sg',\n    'number_of_total_atoms' : 'Natoms',\n    'percent_atom_al' : 'x_Al',\n    'percent_atom_ga' : 'x_Ga',\n    'percent_atom_in' : 'x_In',\n    'lattice_vector_1_ang' : 'a',\n    'lattice_vector_2_ang' : 'b',\n    'lattice_vector_3_ang' : 'c',\n    'lattice_angle_alpha_degree' : 'alpha',\n    'lattice_angle_beta_degree' : 'beta',\n    'lattice_angle_gamma_degree' : 'gamma',\n    'formation_energy_ev_natom' : 'E',\n    'bandgap_energy_ev' : 'Eg'}, inplace=True)\n\ntest.rename(columns={\n    'spacegroup' : 'sg',\n    'number_of_total_atoms' : 'Natoms',\n    'percent_atom_al' : 'x_Al',\n    'percent_atom_ga' : 'x_Ga',\n    'percent_atom_in' : 'x_In',\n    'lattice_vector_1_ang' : 'a',\n    'lattice_vector_2_ang' : 'b',\n    'lattice_vector_3_ang' : 'c',\n    'lattice_angle_alpha_degree' : 'alpha',\n    'lattice_angle_beta_degree' : 'beta',\n    'lattice_angle_gamma_degree' : 'gamma',\n}, inplace=True)\n\n\ntarget = [\n    'E',\n    'Eg']\n\nall_data = pd.concat((train, test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7059f90cba75e10c0784c80c794c2114a000862c",
        "_cell_guid": "8b4a483f-db10-4ee7-bbc0-039f63c08e6b"
      },
      "cell_type": "markdown",
      "source": "## Retrieve list of elemental Properties\n\n- Many people in the forum commented that adding outside information can add useful features\n- The following code adds many features in order to finally add atomic_density and volume"
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "3edc55226ecdbe930bc1be306361a1bad605eded",
        "_cell_guid": "090e9f8c-c2d6-4698-adc6-43c9e320931e",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def get_prop_list(path_to_element_data):\n    \"\"\"\n    Args:\n        path_to_element_data (str) - path to folder of elemental property files\n    Returns:\n        list of elemental properties (str) which have corresponding .csv files\n    \"\"\"\n    return [f[:-4] for f in os.listdir(path_to_element_data)]\n\n# folder which contains element data\npath_to_element_data = '../input/elemental-properties/'\n# get list of properties which have data files\nproperties = get_prop_list(path_to_element_data)\nprint(sorted(properties))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "db7463bc165482f2ea48b078639e1108b51009bc",
        "_cell_guid": "2c4abdf0-0782-4184-b6fc-4b8dae3bf8ff",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def get_prop(prop, path_to_element_data):\n    \"\"\"\n    Args:\n        prop (str) - name of elemental property\n        path_to_element_data (str) - path to folder of elemental property files\n    Returns:\n        dictionary of {element (str) : property value (float)}\n    \"\"\"\n    fin = os.path.join(path_to_element_data, prop+'.csv')\n    with open(fin) as f:\n        all_els = {line.split(',')[0] : float(line.split(',')[1][:-1]) for line in f}\n        my_els = ['Al', 'Ga', 'In']\n        return {el : all_els[el] for el in all_els if el in my_els}\n\n# make nested dictionary which maps {property (str) : {element (str) : property value (float)}}\nprop_dict = {prop : get_prop(prop, path_to_element_data) for prop in properties}\nprint('The mass of aluminum is %.2f amu' % prop_dict['mass']['Al'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "collapsed": true,
        "_uuid": "3539872565448ba24254fd4ccc02d8ddf3964a7d",
        "_cell_guid": "5c1c677b-e454-4e32-92e6-b45548f94c96",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# average each property using the composition\n\ndef avg_prop(x_Al, x_Ga, x_In, prop):\n    \"\"\"\n    Args:\n        x_Al (float or DataFrame series) - concentration of Al\n        x_Ga (float or DataFrame series) - concentration of Ga\n        x_In (float or DataFrame series) - concentration of In\n        prop (str) - name of elemental property\n    Returns:\n        average property for the compound (float or DataFrame series), \n        weighted by the elemental concentrations\n    \"\"\"\n    els = ['Al', 'Ga', 'In']\n    concentration_dict = dict(zip(els, [x_Al, x_Ga, x_In]))\n    return np.sum(prop_dict[prop][el] * concentration_dict[el] for el in els)\n\n# add averaged properties to DataFrame\nfor prop in properties:\n    all_data['_'.join(['avg', prop])] = avg_prop(all_data['x_Al'], \n                                                 all_data['x_Ga'],\n                                                 all_data['x_In'],\n                                                 prop)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "collapsed": true,
        "_uuid": "9a8e5ea1581c4f6239234b0cbc1364cc0b9bfef7",
        "_cell_guid": "414f1a71-749f-44aa-bbd1-5349b281d668",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# calculate the volume of the structure\n\ndef get_vol(a, b, c, alpha, beta, gamma):\n    \"\"\"\n    Args:\n        a (float) - lattice vector 1\n        b (float) - lattice vector 2\n        c (float) - lattice vector 3\n        alpha (float) - lattice angle 1 [radians]\n        beta (float) - lattice angle 2 [radians]\n        gamma (float) - lattice angle 3 [radians]\n    Returns:\n        volume (float) of the parallelepiped unit cell\n    \"\"\"\n    return a*b*c*np.sqrt(1 + 2*np.cos(alpha)*np.cos(beta)*np.cos(gamma)\n                           - np.cos(alpha)**2\n                           - np.cos(beta)**2\n                           - np.cos(gamma)**2)\n\n# convert lattice angles from degrees to radians for volume calculation\nlattice_angles = ['alpha', 'beta', 'gamma']\nfor lang in lattice_angles:\n    all_data['_'.join([lang, 'r'])] = np.pi * all_data[lang] / 180\n    \n# compute the cell volumes \nall_data['vol'] = get_vol(all_data['a'], all_data['b'], all_data['c'],\n                          all_data['alpha_r'], all_data['beta_r'], all_data['gamma_r'])\n\n# calculate the atomic density\nall_data['atomic_density'] = all_data['Natoms'] / all_data['vol']  ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c830806f015cb49fdf720b40b75624c38883ec1e",
        "_cell_guid": "a55e29ec-0fae-44d2-ac1d-ae8ab7c4f82a"
      },
      "cell_type": "markdown",
      "source": "## Imputing/Preprocessing the data for model\n\nSome of the values are actually categorical in Nature and I used that to my advantage.\n"
    },
    {
      "metadata": {
        "_uuid": "4a8309a37d36bac1d318e353245874643bc41115",
        "_cell_guid": "bf610eae-7825-4680-a745-0788c069d7b6",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "all_data.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a59591dc5dc75f85bef0a28d2e697dcd2c1d673e",
        "_cell_guid": "8194646d-15fb-427e-8435-826164d095bb"
      },
      "cell_type": "markdown",
      "source": "### Using Catigorical Features\n- I noticed after looking through that some of the categorical data can group other features\n- I took the categories and plotted a distribution plot of different features grouped by their categories"
    },
    {
      "metadata": {
        "_uuid": "b162a643b138c6c4a11c38402467eeb37f46fa47",
        "_cell_guid": "391d6581-2843-49a4-8f82-77b05c2fee9a",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "for col in ['x_Al', 'x_Ga', 'x_In', 'a', 'b', 'c', 'vol', 'atomic_density']:\n    for x in all_data.sg.unique():\n        sns.distplot(all_data[all_data['sg'] == x][col])\n    plt.title(col)\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bf4b24a3b31ac525e17f5227772cd02f989962d7",
        "_cell_guid": "1f24def8-5293-48bc-91a3-dbbad2806b29"
      },
      "cell_type": "markdown",
      "source": "### Looking above, many features such as (a, b, c) have different distributions depending on their group\n- You can do some simple feature engineering by grouping the training data and adding the mean for \"co-grouped\" numerical data\n- This improved my score slightly"
    },
    {
      "metadata": {
        "_uuid": "488ea69a326ea422c8d75374eda9a4db48f488fc",
        "_cell_guid": "99c1c479-3a80-4eb6-8124-7beb2700d266",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# make new features using averages of the following columns by sg group\navg_cols = ['x_Al','x_Ga','x_In','a','b','c','avg_rs_max','avg_electronegativity',\n            'avg_rp_max','avg_LUMO','avg_IP','avg_rd_max','avg_EA','avg_HOMO',\n            'avg_mass','vol','atomic_density']\n\n\nfor col in avg_cols:\n    new_col = col + \"_avg\"\n    all_data[new_col] = np.nan\n    for group in all_data['sg'].unique():\n        all_data.loc[(all_data['sg'] == group), new_col] = all_data[(all_data['sg'] == group)][col].mean()\n\nprint('Number of Null Values: {}'.format(pd.isnull(all_data[avg_cols]).sum().sum()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "68771197b69fd8a813525be585f467e3c05f84cc",
        "_cell_guid": "3143b7fb-dba6-41e4-89c8-29da416a3a08",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Handle the values with categorical variables using one hot encoding\n# This will create a much more sparse set of variables\n\nall_data[['sg', 'Natoms']] = all_data[['sg', 'Natoms']].astype(str)\nall_data = pd.get_dummies(all_data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cd6ac81d7dd4484ebc2b993b814a8cb021931073",
        "_cell_guid": "620dd3cf-2f0f-443b-afaa-8850b7581893"
      },
      "cell_type": "markdown",
      "source": "### Correcting the distribution of the target variables\n- Both of the target variables didn't have quite a normal distribution.\n- I used the log1p function to fix this.\n- Improved my score slightly"
    },
    {
      "metadata": {
        "_uuid": "ee710f752786dca06b0b078c4a06f6b080b13983",
        "_cell_guid": "bb50bf71-dee2-4e85-a6cd-a4a08a023db0",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#  both of the target variables are skewed a bit\n\nfor col in ['E', 'Eg']:\n    sns.distplot((train[col]))\n    plt.title(col)\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5943ac693d67b61e99313b084a79ebcf01d3917e",
        "_cell_guid": "d95a1eb3-2526-4815-b30a-eb35de2d6e25"
      },
      "cell_type": "markdown",
      "source": "### Preprocessing data for models\n"
    },
    {
      "metadata": {
        "_cell_guid": "9c129c6f-a1f1-4dc4-866d-1388c8d08342",
        "_uuid": "6438fb40d4eb9eda19e5dcd62b8dfb62022a2017",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "all_data.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2fa584b0da6498788d890987d2a75346fcbf7526",
        "_cell_guid": "c4d8c8c0-29fe-4819-a9b9-453b3f93d7c5",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# features to use\nfeatures = ['x_Al', 'x_Ga', 'x_In', 'a', 'b', 'c', 'alpha', 'beta',\n            'gamma', 'vol', 'atomic_density', 'x_Al_avg','x_Ga_avg', 'x_In_avg', 'a_avg',\n            'b_avg', 'c_avg', 'vol_avg', 'atomic_density_avg', 'pca_abc', 'pca_AlGaInDensity',\n            'O_0_0','O_1_0', 'O_2_0', 'O_3_0', 'O_4_0', 'O_5_0', 'Al_0_0', 'Al_1_0', 'Al_2_0', 'Al_3_0', 'Al_4_0', 'Al_5_0', 'Ga_0_0',\n            'Ga_1_0', 'Ga_2_0', 'Ga_3_0', 'Ga_4_0', 'Ga_5_0', 'In_0_0', 'In_1_0',\n            'In_2_0', 'In_3_0', 'In_4_0', 'In_5_0',]\n\n# two different vectors for pca\nvector1 = all_data[['a', 'b', 'c']].values\nvector2 = all_data[['x_Al', 'x_Ga', 'x_In', 'atomic_density_avg']].values\n\n# use pca to add new features\npca = PCA()\npca.fit(vector1)\nall_data['pca_abc'] = pca.transform(vector1)[:,0]\n\npca = PCA()\npca.fit(vector2)\nall_data['pca_AlGaInDensity'] = pca.transform(vector2)[:,0]\n\n# scaling the data. Linear models tend to like more normally distributed\n# I tried training on non-scaled, with slightly worse results\nscale = StandardScaler()\nscaled = scale.fit(all_data[features]).transform(all_data[features])\n\nX_scale = scaled[:train.shape[0]]\nX_scaled_test = scaled[train.shape[0]:]\n\nX_tr = all_data[:train.shape[0]][features].values\nX_te = all_data[train.shape[0]:][features].values\n\ny1 = np.log1p(train['E'])\ny2 = np.log1p(train['Eg'])\n\ny12 = np.column_stack((y1, y2))\n\nX_tr.shape, y1.shape, y2.shape, y12.shape, X_scaled_test.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "df5ab202baf19eba8cde672cd4a4a201bf7427d9",
        "_cell_guid": "46fd2da0-0a46-456a-9dc5-eb6ea9664472"
      },
      "cell_type": "markdown",
      "source": "### The performance metric for this competition"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "0e00f18db948c3b6d283065f9c8c80c1468967be",
        "_cell_guid": "81b0c99e-8d75-4217-9196-e3d8a4122ebb",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# performance matric\ndef rmsle(h, y): \n    \"\"\"\n    Compute the Root Mean Squared Log Error for hypthesis h and targets y\n\n    Args:\n        h - numpy array containing predictions with shape (n_samples, n_targets)\n        y - numpy array containing targets with shape (n_samples, n_targets)\n    \"\"\"\n    \n#     h, y = np.expm1(h), np.expm1(y)\n    \n    return np.sqrt(np.square(np.log(h + 1) - np.log(y + 1)).mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7f52243748646e57ae6cd0ad2d117460585c4316",
        "_cell_guid": "746d41ac-8799-45d7-8d0a-1f00473129fc"
      },
      "cell_type": "markdown",
      "source": "# Modeling: NN, GraidentBoost, LightGBM, and XGBoost"
    },
    {
      "metadata": {
        "_uuid": "446e624cc8f6a68c12b83bcdf33d3f4bd6ed8d30",
        "_cell_guid": "6e742bdb-9208-4bb9-b9a6-93da15f90579"
      },
      "cell_type": "markdown",
      "source": "### Standard Dense Nerual Network Implimentation\n- Use dense layers with Dropout / l2 regularization\n- Best RMSLE: 0.0577"
    },
    {
      "metadata": {
        "_uuid": "ed5a36452e959214d07a137a7fb7c65dbf033688",
        "_cell_guid": "e82cf340-a4ba-430f-a3ae-010c4039cdab",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import keras\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,BatchNormalization, Input",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "e41550b356ea64bc2def204c3516ec5d364b3cb3",
        "_cell_guid": "f949d2ec-3b46-4148-9207-e2b7195bec03",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def get_model(shape):\n    \"\"\" Returns a model of specific shape\n    \"\"\"\n    X_input = Input(shape=(shape,))\n    \n    X = Dense(64, activation='relu')(X_input)\n    X = Dense(64, activation='relu')(X)\n    X = Dropout(0.1)(X)\n    \n    X = Dense(64, activation='relu')(X)\n    X = Dense(64, activation='relu')(X)\n    X = Dropout(0.1)(X)\n    \n    X = Dense(64, activation='relu')(X)\n    X = Dense(64, activation='relu')(X)\n    X = Dropout(0.1)(X)\n    \n    X = Dense(64, activation='relu')(X)\n    X = Dense(64, activation='relu')(X)\n    X = Dropout(0.1)(X)\n    \n    X = Dense(64, activation='relu')(X)\n    X = Dense(64, activation='relu')(X)\n    X = Dense(2, activation='linear')(X)\n\n    \n    return Model(inputs=X_input, outputs=X)\n\n# computes RMSLE from tensorflow\ndef rmsle_K(y, y0):\n    return K.sqrt(K.mean(K.square(tf.log1p(tf.expm1(y)) - tf.log1p(tf.expm1(y0)))))\n\ndef compile_model(shape, lr=0.001):\n    model = get_model(shape)\n    optimizer = Adam(lr=lr, decay=0.001)\n    model.compile(optimizer=optimizer, loss='mse', metrics=[rmsle_K])\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "10790f78b9e55a79f0bd507e42a8b65c47ab1597",
        "_cell_guid": "8635bc83-0937-40f2-9b77-a73366f817e6",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# uncomment below to train the model.\n\n# out = []\n# for idx, label in enumerate([0.002, 0.0022, 0.0024, 0.0026]):\n#     model = compile_model(X_scale.shape[1], lr=label)\n#     evl = model.fit(x=X_scale, y=y12, epochs=600, batch_size=16, verbose=2)\n#     out.append(evl.history.get('rmsle_K'))\n#     print(str(label) + \" rmsle: {}\".format(np.mean(out[idx][180:])))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "384b4ba45c97f0ddf9af73c441730230eeaf92fa",
        "_cell_guid": "684334bb-301e-46d2-b732-38f6aae8e5b1",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Will assess the model using cross validation.\n# This will take a long time depending on your hardwear\ndef assess_nn(X, y, params):\n    \"\"\" Used to access model performance. Returns the mean rmsle score of cross validated data\n    \"\"\"\n    final = []\n    best_iter = [[], []]\n    kfold = KFold(n_splits=2, shuffle=True)\n    out = []\n    for train_index, test_index in kfold.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        model = compile_model(X_scale.shape[1], lr=0.003)\n        model.fit(x=X_scale, y=y12, epochs=600, batch_size=16, verbose=0)\n        h =  model.predict(X_test)\n        e = rmsle(np.expm1(h), np.expm1(y_test))\n        print(e)\n        out.append(e)\n    final.append(np.array(out).mean())\n                      \n    print('y1 best iteration: {}'.format(np.mean(best_iter[0])))\n    print('y2 best iteration: {}'.format(np.mean(best_iter[1])))\n    return(np.array(final).mean(), np.array(final).std())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "4f6d29872902fb095ba89030d4fa798d180f9318",
        "_cell_guid": "f0015e0b-0dbe-4eb2-95ad-bdeaa59b03d8",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# uncomment to assess model\n\n# model = access(X_scale, y12, params)\n# print(model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "802f5039c94e5f3d41dc78edd618b91f686ab936",
        "_cell_guid": "9301b0de-22ce-4ecf-b14d-e8f0a8d083ab",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# if you run the access function above, use the plotting below to plot the last 40 rows of each kfold\n\n# for x in out:\n#     plt.plot(x[-40:])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fff309840e3be5cd75118626763fd16df813298b",
        "_cell_guid": "62ac567f-0e98-4c49-a968-e22320be1fb0"
      },
      "cell_type": "markdown",
      "source": "## Gradient Boosted Regression"
    },
    {
      "metadata": {
        "_uuid": "cd69f35a364ae81c556986c86bb5fe759eff0c2b",
        "_cell_guid": "86135c86-19ae-4b3f-9274-8d365c19cfa3"
      },
      "cell_type": "markdown",
      "source": "### To find values for sklearn \"like\" packages, I used sklearn-optimization for Baysian Optimization\n\n- Easy implimentation of baysian optimization for sklearn-like model objects\n- Multi-threaded, fast, and easy\n- For more controll or other models not like sklearn, I like using another baysian optimization package found here: https://github.com/fmfn/BayesianOptimization"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "f4ff1bc45346daf5a98f19ad30e820017a06cc01",
        "_cell_guid": "66e9ad54-ce4b-4cf3-9c9f-7f9416b67ba0",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# from skopt import BayesSearchCV\n# from skopt.space import Real, Categorical, Integer\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import GradientBoostingRegressor",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "140a19ec99af27a26f47c2dd80b9c1b042d2f674",
        "_cell_guid": "7ce15023-3837-496a-aea0-3f8bea1b03ff"
      },
      "cell_type": "markdown",
      "source": "### Finding Parameters\n\n- For boosted trees, I start by choosing a learning rate in (0.01, 0.001) range\n- Use baysian optimization to find number of estimators for that learning rate. If man estimators are needed, use a larger learning rate\n- Use baysian optimization ot find other parameters"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "e083f1350f1ab8c57f756249f86464abe6fcf9cf",
        "_cell_guid": "b07efc88-ca38-486a-b35b-7c9d210670f0",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# uses baysian optimization to find model parameters\n\n# model = GradientBoostingRegressor(\n#     loss='ls',\n#     learning_rate = 0.0035,\n#     max_depth=23,\n#     n_estimators=30275,\n#     max_features=9,\n#     min_samples_leaf=22,\n#     min_samples_split=15,\n#     min_weight_fraction_leaf=0.0102470171519909\n# )\n\n# search_params = {\n#     \"n_estimators\": Integer(1000, 4000),\n#     'max_depth': Integer(2, 40),\n#     'min_samples_split': Integer(2, 15),\n#     'min_samples_leaf': Integer(2, 50),\n#     'min_weight_fraction_leaf': Real(0., .5),\n#     'max_features': Integer(2, 13)\n# }\n\n# opt = BayesSearchCV(model, search_params, n_iter=50, n_jobs=8, )\n# opt.fit(X_scale, y2)\n# opt.best_params_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "29cb8425ed340681d9f13db54bdf263f12c6e4b5",
        "_cell_guid": "deaf3f19-9c7b-49a8-a67c-07c4446421fa",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# run different model for different Target Variables\n\ngrad_1 = GradientBoostingRegressor(\n                loss='ls',\n                learning_rate = 0.0035,\n                max_depth=7,\n                n_estimators=1120,\n                max_features=7,\n                min_samples_leaf=43,\n                min_samples_split=14,\n                min_weight_fraction_leaf=0.01556)\n\ngrad_2 = GradientBoostingRegressor(\n                loss='ls',\n                learning_rate = 0.0035,\n                max_depth=6,\n                n_estimators=3275,\n                max_features=2,\n                min_samples_leaf=2,\n                min_samples_split=2,\n                min_weight_fraction_leaf=0.08012)\n\ndef assess_grad(X, y_list, model_list):\n    \"\"\" Used to access model performance. Returns the mean rmsle score of cross validated data\n    \"\"\"\n    final = []\n    best_iter = [[], []]\n    for idx, y in enumerate(y_list):\n        kfold = KFold(n_splits=10, shuffle=True)\n        out = []\n        for train_index, test_index in kfold.split(X):\n            X_train, X_test = X[train_index], X[test_index]\n            y_train, y_test = y[train_index], y[test_index]\n            model = model_list[idx]\n            model.fit(X_train, y_train)\n            h =  model.predict(X_test)\n            e = rmsle(np.expm1(h), np.expm1(y_test))\n            print(e)\n            out.append(e)\n        final.append(np.array(out).mean())\n                      \n    return(np.array(final).mean(), np.array(final).std())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1b3a18f212fbca1edf7a300132200178c54471a4",
        "_cell_guid": "5e9c9071-b8f2-4a7a-b68f-eda8d0a78458",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model = assess_grad(X_tr, [y1, y2], [grad_1, grad_2])\nprint(\"Model RMSLE: {}, std: {}\".format(model[0], model[1]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5de743cb241ecb43330b624e1bbb12b46ee6f4b6",
        "_cell_guid": "798ddf86-3b9b-425d-ad7e-d1b74df0b2d3"
      },
      "cell_type": "markdown",
      "source": "## Light GBM, a Microsoft package, and great implimentation of a boosted tree model¶\nCreated and maintained by Microsoft.\nLatest commits show a lot of activity\nProven to be very fast and scalable\nOffers many more parameters for fine tuning models then similar approaches\nHas sklearn Like objects available"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "f7a2930eefc893bcdfbb5c1e83aa809f0a2269e7",
        "_cell_guid": "ca29faf0-27b9-4292-8506-003bc8ad8f44",
        "trusted": false
      },
      "cell_type": "code",
      "source": "import lightgbm as lgb\nfrom lightgbm import Dataset",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "062c09a96442624b56fd42958cba2aef69bdb9d1",
        "_cell_guid": "ff46b05e-58a1-49b6-9021-7b3aa5009f63"
      },
      "cell_type": "markdown",
      "source": "### Finding Parameters\n\n- For boosted trees, I start by choosing a learning rate in (0.01, 0.001) range\n- Use baysian optimization to find number of estimators for that learning rate. If man estimators are needed, use a larger learning rate\n- Use baysian optimization ot find other parameters"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "6e5e8facf918dc8b8e98c2f63c4e7bbe35d58845",
        "_cell_guid": "7b8170c9-fc61-438f-9216-b5b4284285c2",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# find useful parameters for model\n\n# model = lgb.LGBMRegressor(\n#                 objective= 'regression',\n#                 boosting_type= 'gbdt',\n#                 learning_rate= 0.002,\n#                 num_boost_round = 2000,\n#                 num_threads=1,\n#                 bagging_fraction=0.50173,\n#                 bagging_freq= 14,\n#                 feature_fraction= 0.62509,\n#                 lambda_l2= 0.0086298,\n# #                 max_depth=10,\n#                 num_leaves=196\n#             )\n\n# search_params = {\n#     'boosting_type': 'gbdt',\n#         'objective': 'regression',\n#         'metric': {'rmse', 'rmsle'},\n#         'max_depth': Integer(20, 100),\n#         'num_leaves': Integer(100, 200),\n#         'learning_rate': 0.010,\n#         'feature_fraction': Real(0.5, 1.),\n#         'bagging_fraction': Real(0.5, 1),\n#         'bagging_freq': Integer(5, 15),\n#         'num_threads': -1,\n#         'lambda_l2': Real(.00001, 0.01, 'log_normal'),\n#             'lambda_l1': Real(.00001, 0.01, 'log_normal'),\n#     'num_iterations': Integer(1000, 4000)\n# \n\n# opt = BayesSearchCV(model, search_params, n_iter=50, n_jobs=-1, verbose=0)\n# opt.fit(X_scale, y1)\n# opt.best_params_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "efb977a373a40cba60fbb7751d4e521095db0c04",
        "_cell_guid": "22b17a03-46e5-48d1-9417-53f15fe84531",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Run accessment using parameters found below\n\nlgb_1 = lgb.LGBMRegressor(\n                objective= 'regression',\n                boosting_type= 'gbdt',\n                learning_rate= 0.002,\n                n_estimators = 2000,\n                num_threads=3,\n                bagging_fraction=0.56369,\n                bagging_freq= 14,\n                feature_fraction= 0.88868,\n                lambda_l2= 0.0091689,\n                max_depth=20,\n                )\n\nlgb_2 = lgb.LGBMRegressor(\n                objective= 'regression',\n                boosting_type= 'gbdt',\n                learning_rate= 0.002,\n                n_estimators = 2838,\n                num_threads=3,\n                bagging_fraction=0.50173,\n                bagging_freq= 14,\n                feature_fraction= 0.62509,\n                lambda_l2= 0.0086298,\n                max_depth=20,\n                )\n\n# This access uses the built in early stopping functions that make light gmb so great.\n\ndef assess_light(X, y_list, model_list):\n    \"\"\" Used to access model performance. Returns the mean rmsle score of cross validated data\n    \"\"\"\n    final = []\n    best_iter = [[], []]\n    for idx, y in enumerate(y_list):\n        kfold = KFold(n_splits=10, shuffle=True)\n        out = []\n        for train_index, test_index in kfold.split(X):\n            X_train, X_test = X[train_index], X[test_index]\n            y_train, y_test = y[train_index], y[test_index]\n            model = model_list[idx]\n            model.fit(X_train, y_train)\n            h =  model.predict(X_test)\n            e = rmsle(np.expm1(h), np.expm1(y_test))\n            print('RMSLE: {}'.format(e))\n            out.append(e)\n        final.append(np.array(out).mean())\n\n    return(np.array(final).mean(), np.array(final).std())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9f43f05625aa0207f1c886513913df8bf31f84e7",
        "_cell_guid": "a57fc004-4297-41cf-b27c-2020b60dd1b3",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model = assess_light(X_scale, [y1, y2], [lgb_1, lgb_2])\nprint(\"Model RMSLE: {}, std: {}\".format(model[0], model[1]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "53c0159af38c7585acecdeec638980f8e6b50927",
        "_cell_guid": "24583aa6-8ffd-4efe-b00e-8015895eb40a"
      },
      "cell_type": "markdown",
      "source": "### Light GBM also has a useful \"early-stopping\" option. The following access function will use this function\n\n- By experimenting and using bayes optimization, I was able to get better results by manually setting the number of rounds"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "139f9b36e8dbb5161b1a41ba3e8177aa5b4f02ba",
        "_cell_guid": "c4d72483-e84e-416d-9136-7ca97b5a4d9b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "\ndef assess_early_stop_light(X, y_list, params):\n    \"\"\" Used to access model performance. Returns the mean rmsle score of cross validated data\n    \"\"\"\n    final = []\n    best_iter = [[], []]\n    for idx, y in enumerate(y_list):\n        kfold = KFold(n_splits=10, shuffle=True)\n        out = []\n        for train_index, test_index in kfold.split(X):\n            X_train, X_test = X[train_index], X[test_index]\n            y_train, y_test = y[train_index], y[test_index]\n            lgb_train = lgb.Dataset(X_train, y_train)\n            lgb_valid = lgb.Dataset(X_test, y_test, reference=lgb_train)\n            model = lgb.train(params[idx],\n                            lgb_train,\n                            num_boost_round=1000000,\n                            valid_sets=[lgb_valid],\n                            early_stopping_rounds=100,\n                            verbose_eval=0) \n            best_iter[idx].append(model.best_iteration)\n            h =  model.predict(X_test, num_iteration=model.best_iteration)\n            e = rmsle(np.expm1(h), np.expm1(y_test))\n            print('RMSLE: {}'.format(e))\n            out.append(e)\n        final.append(np.array(out).mean())\n                      \n    print('y1 best iteration: {}'.format(np.mean(best_iter[0])))\n    print('y2 best iteration: {}'.format(np.mean(best_iter[1])))\n    return(np.array(final).mean(), np.array(final).std())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "08c34b98eccb105ee0bc14e0e08d77dec72b304a",
        "_cell_guid": "2bbf498d-805d-4ae6-9fbf-3ab1b72c0ddf",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\nparams1 = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'rmse', 'rmsle'},\n            'max_depth': 10,\n            'learning_rate': 0.010,\n            'feature_fraction': 0.8632,\n            'bagging_fraction': 0.8759,\n            'bagging_freq': 4,\n            'verbose': 0,\n            'verbose_eval':0,\n            'num_threads': 3,\n            'lambda_l2': 0.0005597442104287973,\n            'lambda_l1': 0.00015997163552092318 \n        }\n\nparams2 = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'rmse', 'rmsle'},\n            'learning_rate': 0.010,\n            'verbose': 0,\n            'verbose_eval':0,\n            'num_threads': 3,\n            'bagging_fraction': 0.9311539021934098,\n             'bagging_freq': 15,\n             'feature_fraction': 0.9989744117209727,\n             'lambda_l1': 8.337666829263869e-05,\n             'lambda_l2': 0.005541689229153562,\n             'max_depth': 19\n        }\n\nmodel = assess_early_stop_light(X_tr, [y1, y2], [params1, params2])\nprint(\"Model RMSLE: {}, std: {}\".format(model[0], model[1]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c3a88989eb71eeaa2b6883abdec61138f0c10775",
        "_cell_guid": "ea3a0787-5cc8-4117-9f51-5c578ff6d7a7"
      },
      "cell_type": "markdown",
      "source": "## XGBoost, one of the most popular and successful boosting tree algorithms\n- Available in many different languages\n- Very fast and scalable\n- Often wins many different Kaggle competions"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "b4a40647b7ca4fe1b7b8c7f3fcae7b598474592b",
        "_cell_guid": "ff1fda3f-968c-413e-a863-02c970d34bd5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from xgboost import XGBRegressor",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "7b475342ea372fe057ed6643918edc8fa2c63ee1",
        "_cell_guid": "deb192f7-7579-4391-bc67-889ffcb1d3b6",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# find useful parameters for model\n\n# model = XGBRegressor(\n#     silent=True,\n#     learning_rate= 0.0050,\n#     n_jobs=1\n# )\n\n# search_params1 = {\n\n#     'n_estimators': Integer(1804, 1806),\n#     'gamma': Real(0, 100),\n#     'max_depth' : Integer(15, 100),\n#     'min_child_weight': Integer(10, 1000),\n#     'max_delta_step': Integer(1, 100),\n#     'subsample': Real(0, 1),\n#     'colsample_bytree': Real(0.0001, 1),\n#     'colsample_bylevel': Real(0.0001, 1),\n#     'reg_lambda': Real(0.000000001, 1.0),\n# }\n\n# xgb_params_1 = {\n#     'learning_rate':0.005,\n#     'n_jobs':-1,\n#     'n_estimators': 1804,\n#     'gamma': 0.0,\n#     'subsample': 0.749700,\n#     'colsample_bytree': 1.0,\n#     'colsample_bylevel': 0.2790166932949295,\n#     'max_delta_step': 1,\n#     'max_depth': int(15),\n#     'min_child_weight': 30,\n#     'reg_lambda': 1e-09,\n#     'silent': True,\n#     'n_jobs': 8}\n\n# search_params2 = {\n\n#     'n_estimators': Integer(2383, 2400),\n# #     'gamma': Real(0, 100),\n#     'max_depth' : Integer(15, 100),\n#     'min_child_weight': Integer(10, 100),\n#     'max_delta_step': Integer(1, 200),\n#     'subsample': Real(0, 1),\n#     'colsample_bytree': Real(0.01, 1),\n#     'colsample_bylevel': Real(0.01, 1),\n#     'reg_lambda': Real(0.0000001, 0.5),\n    \n# }\n\n# xgb_params_2 = {\n#     'learning_rate':0.005,\n#     'n_jobs':-1,\n#     'n_estimators': 2383,\n#     'colsample_bylevel': 0.982522,\n#     'colsample_bytree': 1.0,\n#     'gamma': 0.0,\n#     'max_depth': 15,\n#     'min_child_weight': 63,\n#     'colsample_bytree': 0.4254153401195336,\n#     'max_delta_step': 65,\n#     'reg_lambda': 0.031165789070644215,\n#     'subsample': 0.6831707073621087,\n#     'silent':True,\n#     'n_jobs':8,\n# }\n\n# for params, y, i in zip([search_params1, search_params2], [y1, y2], ['1', '2']):\n#     opt = BayesSearchCV(model, params, n_iter=100, n_jobs=-1, verbose=0)\n#     opt.fit(X_tr, y)\n#     print('y' + i, opt.best_params_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "9648da1b55fd960f86f15e09ff3439cc0745dee5",
        "_cell_guid": "09a4362a-9f42-4745-a2a2-7ff4a4dcef2a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "xgb_1 = XGBRegressor(\n    learning_rate=0.005,\n    n_jobs=3,\n    n_estimators= 1804,\n    gamma= 0.0,\n    subsample= 0.222159,\n    colsample_bytree= 0.5359,\n    colsample_bylevel= 0.19958,\n    max_delta_step= 64,\n    max_depth=28,\n    min_child_weight= 10,\n    reg_lambda=0.33038,\n    silent= True,\n)\n\nxgb_2 = XGBRegressor(\n    learning_rate=0.005,\n    n_jobs=3,\n    n_estimators= 2386,\n    gamma= 0.0,\n    subsample= 0.90919,\n    colsample_bytree= 0.59049,\n    colsample_bylevel= 0.59404,\n    max_delta_step= 99,\n    max_depth=58,\n    min_child_weight= 85,\n    reg_lambda= 0.031165789070644215,\n    silent= True,\n)\ndef assess_xgb(X, y_list, model_num):\n    \"\"\" Used to access model performance. Returns the mean rmsle score of cross validated data\n    \"\"\"\n    final = []\n    best_iter = [[], []]\n    for idx, y in enumerate(y_list):\n        kfold = KFold(n_splits=10, shuffle=True)\n        out = []\n        for train_index, test_index in kfold.split(X):\n            X_train, X_test = X[train_index], X[test_index]\n            y_train, y_test = y[train_index], y[test_index]\n            model = model_num[idx]\n            model.fit(X_train, y_train)\n            h =  model.predict(X_test)\n            e = rmsle(np.expm1(h), np.expm1(y_test))\n            print('RMSLE: {}'.format(e))\n            out.append(e)\n        final.append(np.array(out).mean())\n    return(np.array(final).mean(), np.array(final).std())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f2e83dbd582a8e9c2f686e7b94da57a67a21fc52",
        "_cell_guid": "7268345d-3c77-4851-b1ee-423f8dba8ed1",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model = assess_xgb(X_scale, [y1, y2], [xgb_1, xgb_2])\nprint(\"Model RMSLE: {}, std: {}\".format(model[0], model[1]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1e84e120cc0e1e31e29f83412b5fdc195855938e",
        "_cell_guid": "e1d6cf3e-638f-493b-9f31-0f18e18bd085"
      },
      "cell_type": "markdown",
      "source": "## Early Stopping with XGBoost"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "f9099321c1fb3cc99a39ddcdbdb027653446ae61",
        "_cell_guid": "cd0685fa-7776-49d2-a2d5-a7b5f641763b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from xgboost import DMatrix\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\n\nxgb_param_1 = {\n    'learning_rate': 0.005,\n    'n_jobs': 3,\n    'gamma': 0.0,\n    'subsample': 0.222159,\n    'colsample_bytree': 0.5359,\n    'colsample_bylevel': 0.19958,\n    'max_delta_step': 64,\n    'max_depth': 28,\n    'min_child_weight': 10,\n    'reg_lambda': 0.33038,\n    'silent':  True\n}\n    \nxgb_param_2 = {\n    'learning_rate': 0.005,\n    'n_jobs': 3,\n    'gamma': 0.0,\n    'subsample': 0.90919,\n    'colsample_bytree': 0.59049,\n    'colsample_bylevel': 0.59404,\n    'max_delta_step': 99,\n    'max_depth': 58,\n    'min_child_weight': 85,\n    'reg_lambda': 0.031165789070644215,\n    'silent': True\n}\n\ndef assess_early_cv_stop_xgb(X, y_list, param_list):\n    \"\"\" Used to access model performance. Returns the mean rmsle score of cross validated data\n    \"\"\"\n    final = []\n    for idx, y in enumerate(y_list):\n        kfold = KFold(n_splits=10, shuffle=True)\n        out = []\n        for train_index, test_index in kfold.split(X):\n            X_iter, X_test = X[train_index], X[test_index]\n            y_iter, y_test = y[train_index], y[test_index]\n            X_train, X_valid, y_train, y_valid = train_test_split(X_iter, y_iter, test_size=0.3)\n            \n            xgb_train = DMatrix(X_train, label=y_train)\n            xgb_valid = DMatrix(X_valid, y_valid)\n            xgb_test = DMatrix(X_test, y_test)\n            watchlist = [ (xgb_train,'train'), (xgb_valid, 'valid') ]\n            model = xgb.train(param_list[idx], xgb_train, 1000000,\n                              watchlist, early_stopping_rounds=20, verbose_eval=False)\n            \n            h =  model.predict(xgb_test)\n            e = rmsle(np.expm1(h), np.expm1(y_test))\n            print('RMSLE: {}'.format(e))\n            out.append(e)\n        final.append(np.array(out).mean())\n    return(np.array(final).mean(), np.array(final).std())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "34d6b1cd3a2eba597725dff9bb800b534295078c",
        "_cell_guid": "ae23744e-af27-4567-92d0-2f2784c1b120",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model = assess_early_cv_stop_xgb(X_tr, [y1, y2], [xgb_param_1, xgb_param_2])\nprint(\"Model RMSLE: {}, std: {}\".format(model[0], model[1]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c4841ff36e073dc06f4f075e41df66b0b1ae9942",
        "_cell_guid": "f987dbc2-184e-443a-b51d-c8c0190e8e76"
      },
      "cell_type": "markdown",
      "source": "## Catboost, another great boosting regressor"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "07c345ac9ea8c34603c803ad1f73a771f46cc02c",
        "_cell_guid": "f5a0fd4d-03b3-49f6-9589-ff37f0a5cfee",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from catboost import CatBoostRegressor",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "d78449356aeaf48348bf791dbfa35c35b3678a17",
        "_cell_guid": "4d5992a7-f4f0-4519-acb8-4351d1cefc92",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# I found these parameterw worked for both y variables\ncat_1 = CatBoostRegressor(iterations=2300,\n                          learning_rate=0.020,\n                          depth=5,\n                          loss_function='RMSE',\n                          eval_metric='RMSE',\n                          od_type='Iter',\n                          od_wait=50,\n                         )\n\ndef assess_cat(X, y_list, model_num):\n    \"\"\" Used to access model performance. Returns the mean rmsle score of cross validated data\n    \"\"\"\n    final = []\n    best_iter = [[], []]\n    for idx, y in enumerate(y_list):\n        kfold = KFold(n_splits=10, shuffle=True)\n        out = []\n        for train_index, test_index in kfold.split(X):\n            X_train, X_test = X[train_index], X[test_index]\n            y_train, y_test = y[train_index], y[test_index]\n            model = model_num[idx]\n            model.fit(X_train, y_train, verbose=False)\n            h =  model.predict(X_test)\n            e = rmsle(np.expm1(h), np.expm1(y_test))\n            print('RMSLE: {}'.format(e))\n            out.append(e)\n        final.append(np.array(out).mean())\n    return(np.array(final).mean(), np.array(final).std())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d7eaaf81ea68efcdee67b39e7ee32a5ca34a1e47",
        "_cell_guid": "dfc375a1-411f-4477-a64a-daa605ee488d",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model = assess_cat(X_tr, [y1, y2], [cat_1, cat_1])\nprint(\"Model RMSLE: {}, std: {}\".format(model[0], model[1]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0d22430cc6085b3a384f112cb69cdf116b4f5fb7",
        "_cell_guid": "0535d676-a370-48d8-8867-f9a26537fd82"
      },
      "cell_type": "markdown",
      "source": "# Catboost has another great feature in built in validation\n- models are validated and the best model is returned for pedictions\n- you have to split up the data more times, so more data is necessary to make this work"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "7386054d0ea484be1bc916494037a0c9147c6af2",
        "_cell_guid": "8d734088-eea1-453f-9cc8-4aa5d392fb05",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\ncatboost_cv = CatBoostRegressor(iterations=1200,\n                            learning_rate=0.03,\n                            depth=5,\n                            loss_function='RMSE',\n                            eval_metric='RMSE',\n                            random_seed=99,\n                            od_type='Iter',\n                            od_wait=50)\n    \ndef assess_cv_catboost(X, y_list):\n    \"\"\" Used to access model performance. Returns the mean rmsle score of cross validated data\n    \"\"\"\n    final = []\n    best_iter = [[], []]\n    for idx, y in enumerate(y_list):\n        kfold = KFold(n_splits=10, shuffle=True)\n        out = []\n        for train_index, test_index in kfold.split(X):\n            # splitting the data up into train, test, and valid sets\n            X_iter, X_test = X[train_index], X[test_index]\n            y_iter, y_test = y[train_index], y[test_index]\n            X_train, X_valid, y_train, y_valid = train_test_split(X_iter, y_iter, test_size=0.3)\n            model =  catboost_cv\n            model.fit(X_train, y_train,\n                      eval_set=(X_valid, y_valid),\n                      use_best_model=True,\n                      verbose=False)\n            h =  model.predict(X_test)\n            e = rmsle(np.expm1(h), np.expm1(y_test))\n            print('RMSLE: {}'.format(e))\n            out.append(e)\n        final.append(np.array(out).mean())\n    return(np.array(final).mean(), np.array(final).std())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6f9aed11d4e6f1516159dcf78ae53dfa0141f48c",
        "_cell_guid": "911f4036-3de6-42ed-98ef-ae9a10b7d900",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model = assess_cv_catboost(X_tr, [y1, y2])\nprint(\"Model RMSLE: {}, std: {}\".format(model[0], model[1]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4fb51da58cecec88d91d852d8c7e63fb9b4dc7c3",
        "_cell_guid": "1ffcd5b8-e23c-4643-ace9-7e2ee0aab019"
      },
      "cell_type": "markdown",
      "source": "## Model Stacking,  combining many models to improve accuracy\n\n- Stacking Meta Models use 2 tiers\n- First tier uses \"base models\" to predict y values\n- Second tier uses \"meta models\" on the predicted values to as the final predictor\n\nFind more information on this stacking approach, here is a great Kaggle blog about it: http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "ccecb2d184c528eb43c6b71df44acea44c7054cb",
        "_cell_guid": "359adf6d-5075-48fd-a2cf-9a4d1eb34878",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import  LinearRegression\nfrom sklearn.base import clone\n\nclass StackingAveragedModels():\n    def __init__(self, base_models, meta_model, n_folds=15):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                try:\n                    instance.fit(X[train_index], y[train_index], verbose=False)\n                except:\n                    instance.fit(X[train_index], y[train_index])\n\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e8af696d1fbdb059ec2fe6781bba505d78fc7fc8",
        "_cell_guid": "81fa15f5-c7ef-41e0-a2ad-fd9203bec68e"
      },
      "cell_type": "markdown",
      "source": "# Below is Another Stacked Model, but with each model using their built in version of early stopping\n- This requires you to further split up the data, which for this problem might be worse for training"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "418c617e7e42ade7f39e510c32ce1682baedb92f",
        "_cell_guid": "74413b80-dacb-4f44-8c84-0d6fef8792a9",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# stacking with early stopping for all models\n\nfrom sklearn.linear_model import  LinearRegression\nfrom sklearn.base import clone\nimport sklearn\nimport xgboost\nimport catboost\nimport lightgbm\n\nclass StackingStoppingAveragedModels():\n    def __init__(self, base_models, meta_model, n_folds=10):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n\n                 # all models support early stopping in one way and need a valid set for it\n                X_train, X_valid, y_train, y_valid = train_test_split(X[train_index],\n                                                                      y[train_index],\n                                                                      test_size=0.3)\n                instance = clone(model)\n                \n                if type(instance) == sklearn.ensemble.gradient_boosting.GradientBoostingRegressor:\n                    instance.min_impurity_decrease = 0.01\n                    \n                    # use all data since grad boosting's \"early stopping\" is a parameter on leafs\n                    instance.fit(X[train_index], y[train_index])\n                    \n                elif type(instance) == xgboost.sklearn.XGBRegressor:\n                    instance.n_estimators = 10000000\n                    instance.fit(X_train, y_train, early_stopping_rounds=50, eval_metric=\"rmse\",\n                                 eval_set=[(X_valid, y_valid)], verbose=False)\n                    \n                elif type(instance) == catboost.core.CatBoostRegressor:\n                        instance.fit(X_train, y_train, eval_set=[X_valid, y_valid],\n                                     use_best_model=True, verbose=False)\n                        \n                elif type(instance) == lightgbm.sklearn.LGBMRegressor:\n                        instance.n_estimators = 10000000\n                        instance.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],\n                                     early_stopping_rounds=100, verbose=False)\n                    \n                self.base_models_[i].append(instance)\n                \n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "collapsed": true,
        "_uuid": "003281654afc3634ad0dfa2cec401aad63fde489",
        "_cell_guid": "25792bc0-15df-4fda-8804-f81f1c3b916c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# stacking for y1\ncat_1 = CatBoostRegressor(iterations=1400,\n                            learning_rate=0.03,\n                            depth=5,\n                            loss_function='RMSE',\n                            eval_metric='RMSE',\n                            od_type='Iter',\n                            od_wait=50)\nxgb_1 = XGBRegressor(\n    learning_rate=0.005,\n    n_jobs=3,\n    n_estimators= 1804,\n    gamma= 0.0,\n    subsample= 0.222159,\n    colsample_bytree= 0.5359,\n    colsample_bylevel= 0.19958,\n    max_delta_step= 64,\n    max_depth=28,\n    min_child_weight= 10,\n    reg_lambda=0.33038,\n    silent= True)\n\nlgb_1 = lgb.LGBMRegressor(\n                objective= 'regression',\n                boosting_type= 'gbdt',\n                learning_rate= 0.002,\n                n_estimators = 2000,\n                num_threads=3,\n                bagging_fraction=0.56369,\n                bagging_freq= 14,\n                feature_fraction= 0.88868,\n                lambda_l2= 0.0091689,\n                max_depth=96,\n                )\n\ngrad_1 = GradientBoostingRegressor(\n                loss='ls',\n                learning_rate = 0.0035,\n                max_depth=7,\n                n_estimators=1120,\n                max_features=7,\n                min_samples_leaf=43,\n                min_samples_split=14,\n                min_weight_fraction_leaf=0.01556)\nlinear = LinearRegression()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "ee4d20df43af9ec5c1ce18906a50d0e3214c8a6e",
        "_cell_guid": "38dcd896-729f-41e1-bf2b-85179c5e8d07",
        "trusted": false
      },
      "cell_type": "code",
      "source": "cat_2 = CatBoostRegressor(iterations=1200,\n                            learning_rate=0.03,\n                            depth=5,\n                            loss_function='RMSE',\n                            eval_metric='RMSE',\n                            od_type='Iter',\n                            od_wait=50)\nlgb_2 = lgb.LGBMRegressor(\n                objective= 'regression',\n                boosting_type= 'gbdt',\n                learning_rate= 0.002,\n                n_estimators = 2838,\n                num_threads=3,\n                bagging_fraction=0.50173,\n                bagging_freq= 14,\n                feature_fraction= 0.62509,\n                lambda_l2= 0.0086298,\n                max_depth=20\n                )\n\ngrad_2 = GradientBoostingRegressor(\n                loss='ls',\n                learning_rate = 0.0035,\n                max_depth=6,\n                n_estimators=3275,\n                max_features=2,\n                min_samples_leaf=2,\n                min_samples_split=2,\n                min_weight_fraction_leaf=0.08012)\n\nxgb_2 = XGBRegressor(\n    learning_rate=0.005,\n    n_jobs=3,\n    n_estimators= 2386,\n    gamma= 0.0,\n    subsample= 0.90919,\n    colsample_bytree= 0.59049,\n    colsample_bylevel= 0.59404,\n    max_delta_step= 99,\n    max_depth=58,\n    min_child_weight= 85,\n    reg_lambda= 0.031165789070644215,\n    silent= True,\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "b77f4b875775e335725d0a3401ef46334d8dd933",
        "_cell_guid": "3c99aa3d-529d-4c4f-b080-10e1d2bf8ec6",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# without early stopping\nstacked_1 = StackingAveragedModels(base_models=[xgb_1, lgb_1, grad_1, cat_1], meta_model=linear)\nstack_1 = stacked_1.fit(X_scale, y1)\n\nstacked_2 = StackingAveragedModels(base_models=[xgb_2, lgb_2, grad_2, cat_1], meta_model=linear)\nstack_2 = stacked_2.fit(X_scale, y2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "32e1786218cdb2794d2c1a60452e6baf6a8b03da",
        "_cell_guid": "ba2e7fae-9a41-4a86-b66c-cd5e64e5e18b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# with early stopping\n# uncomment to run this\n\n# stacked_stop_1 = StackingStoppingAveragedModels(base_models=[xgb_1, lgb_1, grad_1, cat_1], meta_model=linear)\n# stack_1 = stacked_stop_1.fit(X_scale, y1)\n\n# stacked_stop_2 = StackingStoppingAveragedModels(base_models=[xgb_2, lgb_2, grad_2, cat_1], meta_model=linear)\n# stack_2 = stacked_stop_2.fit(X_scale, y2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b66fd23e9ae23facbdf9f68c5d8b1ea601472694",
        "_cell_guid": "1fb78d30-4f24-4ef2-99bd-22c0c511ccff"
      },
      "cell_type": "markdown",
      "source": "# Create Submission"
    },
    {
      "metadata": {
        "_cell_guid": "aed442a3-a68a-40d8-a345-34a42730b100",
        "_uuid": "abbf115ae1b48a386ca91b50d479bfeba73995db",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "all_data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "24149c135406ea776b9fb1c226e4045bc36c56f0",
        "_cell_guid": "182637dc-3af7-4a04-bb00-4ac285ddc7c3",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "one = stack_1.predict(X_scaled_test)\ntwo = stack_2.predict(X_scaled_test)\n_id = test['id']\n\nsubmit = pd.DataFrame({'id':_id,\n                       'formation_energy_ev_natom':np.expm1(one).flatten(),\n                       'bandgap_energy_ev':np.expm1(two).flatten()})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3c139a37e6c20239e07b257e2c6a20fed324760f",
        "_cell_guid": "4968354a-8ed4-4277-bf87-f8b55ed8cd14",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "submit.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "9d6f31c204d4af841e6205a9e967884beb3e5120",
        "_cell_guid": "7be85418-251d-4eb3-8a0c-490506d3dd7b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "submit.to_csv('submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "579456a448d4173d721681960513bca827ae8ee6",
        "_cell_guid": "6915f84d-e5ec-4350-b1d1-4bb741682537"
      },
      "cell_type": "markdown",
      "source": "Thanks a ton for looking at this Kernel. I will continue to update it as I can. If you have any questions or comments, please let me know."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.6.4",
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}